# ğŸ”¬ Document Similarity Analyzer - Algorithm Flow

Dokumentasi lengkap alur algoritma sentence-level similarity dari upload file sampai hasil API response, dengan mapping ke source code spesifik.

---

## ğŸ“Š Pipeline Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. FILE UPLOAD & EXTRACTION                                 â”‚
â”‚  PDF/DOCX/TXT â†’ Extract Text â†’ Raw String                   â”‚
â”‚  ğŸ“ src/api/file_upload.rs, src/extraction/*.rs             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. SENTENCE SPLITTING                                       â”‚
â”‚  Raw Text â†’ Regex [.!?](?:\s+|$) â†’ Array of Sentences      â”‚
â”‚  ğŸ“ src/sentence/mod.rs                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. NORMALIZATION                                            â”‚
â”‚  Sentences â†’ Lowercase + Remove Punctuation                 â”‚
â”‚  ğŸ“ src/core/normalize.rs                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. TOKENIZATION                                             â”‚
â”‚  Normalized Text â†’ Split Whitespace â†’ Array of Words        â”‚
â”‚  ğŸ“ src/core/tokenize.rs                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. TF CALCULATION (Per Sentence)                            â”‚
â”‚  Words â†’ Count Frequency â†’ HashMap<Word, TF>                â”‚
â”‚  ğŸ“ src/core/tf.rs                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. IDF CALCULATION (Global)                                 â”‚
â”‚  All TF Maps â†’ Document Frequency â†’ HashMap<Word, IDF>      â”‚
â”‚  ğŸ“ src/core/idf.rs                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7. TF-IDF VECTORIZATION                                     â”‚
â”‚  TF Ã— IDF â†’ TF-IDF Vector per Sentence                     â”‚
â”‚  ğŸ“ src/core/vectorize.rs (via sentence_pipeline.rs)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  8. COSINE SIMILARITY                                        â”‚
â”‚  Vector A Â· Vector B â†’ Similarity Score (0.0 - 1.0)        â”‚
â”‚  ğŸ“ src/core/similarity.rs (via sentence_pipeline.rs)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  9. THRESHOLD FILTERING                                      â”‚
â”‚  Keep Only: Similarity â‰¥ 0.70                              â”‚
â”‚  ğŸ“ src/core/sentence_pipeline.rs                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  10. GLOBAL SIMILARITY                                       â”‚
â”‚  Average All Matches â†’ Global Score per Doc Pair            â”‚
â”‚  ğŸ“ src/core/sentence_pipeline.rs                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  11. JSON RESPONSE                                           â”‚
â”‚  Matches + Global Similarity + Metadata                     â”‚
â”‚  ğŸ“ src/models/sentence_analysis.rs                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” Step-by-Step Details

### **Step 1: File Upload & Extraction**

**Files:**

- `src/api/file_upload.rs` - Multipart upload handler
- `src/extraction/pdf.rs` - PDF extraction
- `src/extraction/docx.rs` - DOCX extraction
- `src/extraction/txt.rs` - TXT extraction

**Input:**

```http
POST /api/analyze
Content-Type: multipart/form-data

files[]: paper.pdf
files[]: reference.docx
threshold: 0.7
```

**Process:**

```rust
// Validate files (max 5, min 2, max 10MB/file, max 50MB total)
// Extract text based on file type
match file_type {
    PDF => extract_pdf(&bytes),   // pdf-extract library
    DOCX => extract_docx(&bytes), // docx-rs library
    TXT => String::from_utf8(&bytes)
}
```

**Output:**

```rust
Vec<(String, String)> // [(filename, raw_text), ...]
```

**Example:**

```rust
[
    ("paper.pdf", "AI is powerful. ML enables analytics."),
    ("reference.docx", "Machine learning is amazing. ML enables analytics.")
]
```

---

### **Step 2: Sentence Splitting**

**File:** `src/sentence/mod.rs`

**Input:**

```rust
"AI is powerful. ML enables analytics. DL uses neural networks."
```

**Process:**

```rust
// Regex: [.!?](?:\s+|$)
// Split on: . ! ? followed by whitespace or end-of-string
split_sentences(text) -> Vec<String>
```

**Output:**

```rust
[
    "AI is powerful.",
    "ML enables analytics.",
    "DL uses neural networks."
]
```

---

### **Step 3: Normalization**

**File:** `src/core/normalize.rs`

**Input:**

```rust
"Machine Learning is POWERFUL!"
```

**Process:**

```rust
text.to_lowercase()           // "machine learning is powerful!"
    .remove_punctuation()     // "machine learning is powerful"
    .trim()                   // "machine learning is powerful"
```

**Output:**

```rust
"machine learning is powerful"
```

---

### **Step 4: Tokenization**

**File:** `src/core/tokenize.rs`

**Input:**

```rust
"machine learning is powerful"
```

**Process:**

```rust
text.split_whitespace()
    .map(|s| s.to_string())
    .collect()
```

**Output:**

```rust
["machine", "learning", "is", "powerful"]
```

---

### **Step 5: TF Calculation (Per Sentence)**

**File:** `src/core/tf.rs`

**Input:**

```rust
["ml", "enables", "analytics"]
```

**Formula:**

```
TF(word) = word_count / total_words_in_sentence
```

**Calculation:**

```rust
Sentence: ["ml", "enables", "analytics"]
Total: 3 words

TF:
- "ml": 1/3 = 0.333
- "enables": 1/3 = 0.333
- "analytics": 1/3 = 0.333
```

**Output:**

```rust
HashMap<String, f32> {
    "ml": 0.333,
    "enables": 0.333,
    "analytics": 0.333
}
```

---

### **Step 6: IDF Calculation (Global)**

**File:** `src/core/idf.rs`

**Input:**

```rust
// All TF maps from all sentences in all documents
Vec<HashMap<String, f32>>
```

**Formula (Smoothed):**

```
IDF(word) = log((N + 1) / (df + 1)) + 1

Where:
- N = total number of sentences
- df = number of sentences containing the word
```

**Example:**

Given **6 total sentences** from 2 documents:

| Word        | df (sentences containing) | Calculation         | IDF  |
| ----------- | ------------------------- | ------------------- | ---- |
| "ml"        | 2                         | ln((6+1)/(2+1)) + 1 | 1.85 |
| "analytics" | 2                         | ln((6+1)/(2+1)) + 1 | 1.85 |
| "ai"        | 1                         | ln((6+1)/(1+1)) + 1 | 2.25 |

**Output:**

```rust
HashMap<String, f32> {
    "ml": 1.85,
    "analytics": 1.85,
    "ai": 2.25,
    // ... all unique words
}
```

---

### **Step 7: TF-IDF Vectorization**

**File:** `src/core/sentence_pipeline.rs` (using `compute_tfidf_vector()`)

**Input:**

```rust
tf: {"ml": 0.333, "enables": 0.333, "analytics": 0.333}
idf: {"ml": 1.85, "enables": 1.85, "analytics": 1.85, ...}
```

**Process:**

```rust
for word in vocabulary {
    tfidf[word] = tf.get(word) * idf.get(word)
}
```

**Example:**

For sentence "ml enables analytics":

| Word      | TF    | IDF  | TF-IDF    |
| --------- | ----- | ---- | --------- |
| analytics | 0.333 | 1.85 | **0.616** |
| enables   | 0.333 | 1.85 | **0.616** |
| ml        | 0.333 | 1.85 | **0.616** |

**Output:**

```rust
HashMap<String, f32> {
    "ml": 0.616,
    "enables": 0.616,
    "analytics": 0.616
}
```

---

### **Step 8: Cosine Similarity**

**File:** `src/core/similarity.rs`

**Input:**

```rust
vector_a: {"ml": 0.616, "enables": 0.616, "analytics": 0.616}
vector_b: {"ml": 0.616, "enables": 0.616, "analytics": 0.616}
```

**Formula:**

```
cosine_similarity = (A Â· B) / (||A|| Ã— ||B||)

Where:
- A Â· B = dot product (sum of element-wise multiplication)
- ||A|| = magnitude = sqrt(sum of squared values)
```

**Calculation:**

```rust
// Dot Product
= 0.616Â² + 0.616Â² + 0.616Â²
= 1.137

// Magnitude A
= sqrt(0.616Â² + 0.616Â² + 0.616Â²)
= 1.066

// Magnitude B
= 1.066 (same as A)

// Cosine Similarity
= 1.137 / (1.066 Ã— 1.066)
= 1.00 (identical!)
```

**Output:**

```rust
f32: 1.00 // Range: 0.0 (different) to 1.0 (identical)
```

---

### **Step 9: Threshold Filtering**

**File:** `src/core/sentence_pipeline.rs` - `compute_sentence_matches()`

**Input:**

```rust
all_similarities: [
    (doc0_sent0, doc1_sent0, 0.45),  // âŒ below threshold
    (doc0_sent1, doc1_sent1, 1.00),  // âœ… above threshold
    (doc0_sent2, doc1_sent2, 0.87),  // âœ… above threshold
]
threshold: 0.70
```

**Process:**

```rust
similarities
    .into_iter()
    .filter(|(_, _, sim)| *sim >= threshold)
    .collect()
```

**Output:**

```rust
Vec<SentenceMatch> [
    SentenceMatch {
        source_doc: "paper.pdf",
        source_sentence_index: 1,
        source_sentence: "ml enables analytics",
        target_doc: "reference.docx",
        target_sentence_index: 1,
        target_sentence: "ml enables analytics",
        similarity: 1.00
    },
    SentenceMatch {
        source_doc: "paper.pdf",
        source_sentence_index: 2,
        target_doc: "reference.docx",
        target_sentence_index: 3,
        similarity: 0.87
    }
]
```

---

### **Step 10: Global Similarity**

**File:** `src/core/sentence_pipeline.rs` - `compute_global_similarities()`

**Input:**

```rust
matches: Vec<SentenceMatch> // from Step 9
```

**Process:**

```rust
// 1. Group by document pair
// 2. Calculate average similarity for each pair
let avg = similarities.iter().sum() / count
```

**Example:**

```rust
Matches between paper.pdf â†” reference.docx:
- Sentence 1 vs 1: 1.00
- Sentence 2 vs 3: 0.87

Global Similarity = (1.00 + 0.87) / 2 = 0.935
```

**Output:**

```rust
Vec<GlobalSimilarity> [
    GlobalSimilarity {
        docA: "paper.pdf",
        docB: "reference.docx",
        score: 0.935
    }
]
```

---

### **Step 11: JSON Response**

**File:** `src/models/sentence_analysis.rs`

**Input:**

```rust
matches: Vec<SentenceMatch>
global_similarities: Vec<GlobalSimilarity>
metadata: AnalysisMetadata
```

**Output:**

```json
{
  "metadata": {
    "documents_count": 2,
    "total_sentences": 6,
    "processing_time_ms": 4,
    "threshold": 0.7
  },
  "matches": [
    {
      "source_doc": "paper.pdf",
      "source_sentence_index": 1,
      "source_sentence": "ml enables analytics",
      "target_doc": "reference.docx",
      "target_sentence_index": 1,
      "target_sentence": "ml enables analytics",
      "similarity": 1.0
    }
  ],
  "global_similarity": [
    {
      "docA": "paper.pdf",
      "docB": "reference.docx",
      "score": 0.935
    }
  ]
}
```

---

## ğŸ“‚ File Structure

```
src/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ file_upload.rs       # Step 1: Upload handler & orchestrator
â”‚   â”œâ”€â”€ server.rs            # Axum server setup
â”‚   â””â”€â”€ error.rs             # Error types
â”‚
â”œâ”€â”€ extraction/
â”‚   â”œâ”€â”€ pdf.rs               # Step 1: PDF â†’ text
â”‚   â”œâ”€â”€ docx.rs              # Step 1: DOCX â†’ text
â”‚   â””â”€â”€ txt.rs               # Step 1: TXT â†’ text
â”‚
â”œâ”€â”€ sentence/
â”‚   â””â”€â”€ mod.rs               # Step 2: Text â†’ sentences
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ normalize.rs         # Step 3: Preprocessing
â”‚   â”œâ”€â”€ tokenize.rs          # Step 4: Text â†’ words
â”‚   â”œâ”€â”€ tf.rs                # Step 5: Term Frequency
â”‚   â”œâ”€â”€ idf.rs               # Step 6: IDF calculation
â”‚   â”œâ”€â”€ vectorize.rs         # Step 7: TF-IDF vector (helper)
â”‚   â”œâ”€â”€ similarity.rs        # Step 8: Cosine similarity
â”‚   â””â”€â”€ sentence_pipeline.rs # Steps 7, 9, 10: Main orchestrator
â”‚
â””â”€â”€ models/
    â””â”€â”€ sentence_analysis.rs # Step 11: Response models
```

---

## ğŸ”„ Complete Data Flow Example

**Input:** 2 files

```
paper.pdf: "AI is powerful. ML enables analytics."
reference.docx: "ML is amazing. ML enables analytics."
```

**Transformation:**

```
Step 1: Extract
â”œâ”€> [("paper.pdf", "AI is powerful. ML enables analytics."),
     ("reference.docx", "ML is amazing. ML enables analytics.")]

Step 2: Split
â”œâ”€> [[["AI is powerful", "ML enables analytics"],
      ["ML is amazing", "ML enables analytics"]]

Step 3: Normalize
â”œâ”€> [["ai is powerful", "ml enables analytics"],
     ["ml is amazing", "ml enables analytics"]]

Step 4: Tokenize
â”œâ”€> [[["ai","is","powerful"], ["ml","enables","analytics"]],
     [["ml","is","amazing"], ["ml","enables","analytics"]]]

Step 5: TF (per sentence)
â”œâ”€> [[{"ai":0.33,"is":0.33,"powerful":0.33},
      {"ml":0.33,"enables":0.33,"analytics":0.33}],
     [{"ml":0.33,"is":0.33,"amazing":0.33},
      {"ml":0.33,"enables":0.33,"analytics":0.33}]]

Step 6: IDF (4 sentences total)
â”œâ”€> {"ai":1.91, "is":1.22, "powerful":1.91, "ml":1.22,
     "enables":1.22, "analytics":1.22, "amazing":1.91}

Step 7: TF-IDF Vectors
â”œâ”€> paper[0]: {"ai":0.631, "is":0.403, "powerful":0.631}
    paper[1]: {"ml":0.403, "enables":0.403, "analytics":0.403}
    ref[0]:   {"ml":0.403, "is":0.403, "amazing":0.631}
    ref[1]:   {"ml":0.403, "enables":0.403, "analytics":0.403}

Step 8: Cosine Similarity (cross-doc only)
â”œâ”€> paper[0] â†” ref[0]: 0.24
    paper[0] â†” ref[1]: 0.0
    paper[1] â†” ref[0]: 0.0
    paper[1] â†” ref[1]: 1.00 âœ…

Step 9: Filter (â‰¥0.70)
â”œâ”€> [Match(paper[1], ref[1], 1.00)]

Step 10: Global
â”œâ”€> paper â†” reference: 1.00

Step 11: Response
â””â”€> {
      "matches": [{"source_doc":"paper.pdf","similarity":1.00,...}],
      "global_similarity": [{"docA":"paper.pdf","score":1.00,...}]
    }
```

---

## ğŸ¯ Key Algorithm Features

### **Sentence-Level TF-IDF**

- TF calculated **per sentence** (not per document)
- More granular detection: knows **which sentence** is plagiarized

### **Global IDF**

- IDF computed from **all sentences** across **all documents**
- Rare words get higher importance

### **Cosine Similarity**

- Measures **angle** between vectors (not distance)
- Range: 0.0 (orthogonal/different) to 1.0 (parallel/identical)
- Works with sparse vectors (many zeros)

### **Threshold Filtering**

- Default: 0.70 (configurable via API)
- Reduces false positives
- Only stores significant matches

### **Parallel Processing (Rayon)**

- Preprocessing (normalize + tokenize)
- TF calculation
- Similarity computation
- Speeds up processing for large documents

---

## ğŸ’¡ Advantages Over Word-Level System

| Aspect          | Word-Level (Old)                | Sentence-Level (New)                                     |
| --------------- | ------------------------------- | -------------------------------------------------------- |
| **Granularity** | "Doc A is 37% similar to Doc B" | "Sentence 14 in Doc A matches sentence 9 in Doc B (91%)" |
| **Frontend**    | Cannot highlight specific text  | Can highlight exact sentences                            |
| **Detection**   | Document-level only             | Sentence-level precision                                 |
| **Use Case**    | General similarity score        | Plagiarism detection with evidence                       |
| **Response**    | Similarity matrix               | Matched sentences + global score                         |

---

## ğŸš€ Testing

Run the complete pipeline:

```bash
cargo test
# Result: 83 tests passed

cargo run --release
# Server: http://0.0.0.0:3000

curl -X POST http://localhost:3000/api/analyze \
  -F "files[]=@test_doc1.txt" \
  -F "files[]=@test_doc2.txt" \
  -F "threshold=0.7"
```

---

**Last Updated:** December 7, 2025
